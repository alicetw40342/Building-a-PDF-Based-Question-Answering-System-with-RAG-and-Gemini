{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQSqt6bpK05lCxHop+qm/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicetw40342/Building-a-PDF-Based-Question-Answering-System-with-RAG-and-Gemini/blob/main/Project3_task4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fPrFmcYobA8p"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def make_rag_prompt(query: str, relevant_passage: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    å°‡ä½¿ç”¨è€…å•é¡Œèˆ‡ç›¸é—œæ®µè½æ•´åˆæˆç”Ÿæˆæ¨¡å‹çš„ prompt\n",
        "    \"\"\"\n",
        "    context = \"\\n\\n\".join(relevant_passage)\n",
        "    prompt = (\n",
        "        f\"Based on the following information:\\n\\n\"\n",
        "        f\"{context}\\n\\n\"\n",
        "        f\"Please answer the following question:\\n{query}\"\n",
        "    )\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å‡è¨­ä½ å‰›å¾ get_relevant_passage() æ‹¿åˆ°ä»¥ä¸‹æ®µè½\n",
        "retrieved_passages = [\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"Neural networks are inspired by the structure of the human brain.\"\n",
        "]\n",
        "\n",
        "# ä½¿ç”¨è€…çš„å•é¡Œ\n",
        "user_query = \"How is AI connected to how the human brain works?\"\n",
        "\n",
        "# å»ºç«‹ prompt\n",
        "prompt = make_rag_prompt(user_query, retrieved_passages)\n",
        "\n",
        "# é¡¯ç¤º prompt\n",
        "print(\"ğŸ“ ç”Ÿæˆçš„ Promptï¼š\\n\")\n",
        "print(prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvlBoMdmbBsg",
        "outputId": "3efddae2-e82f-4339-bb11-1af452057d42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ ç”Ÿæˆçš„ Promptï¼š\n",
            "\n",
            "Based on the following information:\n",
            "\n",
            "Artificial intelligence is transforming the world.\n",
            "\n",
            "Neural networks are inspired by the structure of the human brain.\n",
            "\n",
            "Please answer the following question:\n",
            "How is AI connected to how the human brain works?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers chromadb PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHOAsNLtbC8a",
        "outputId": "00da3e8b-01fa-4ac6-f270-61aadb5ff34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    reader = PdfReader(pdf_path)\n",
        "    return \"\\n\".join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "def split_text(text: str, max_chunk_size: int = 500) -> List[str]:\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "    chunks = []\n",
        "\n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "        if len(para) > max_chunk_size:\n",
        "            sentences = re.split(r'(?<=[.!?]) +', para)\n",
        "            chunk = \"\"\n",
        "            for sentence in sentences:\n",
        "                if len(chunk) + len(sentence) <= max_chunk_size:\n",
        "                    chunk += \" \" + sentence\n",
        "                else:\n",
        "                    chunks.append(chunk.strip())\n",
        "                    chunk = sentence\n",
        "            if chunk:\n",
        "                chunks.append(chunk.strip())\n",
        "        else:\n",
        "            chunks.append(para)\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "ezqKih2-b2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import chromadb\n",
        "\n",
        "# åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹\n",
        "local_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "class EmbeddingUtils:\n",
        "    @staticmethod\n",
        "    def get_embedding(text: str, model=\"local\") -> List[float]:\n",
        "        return local_model.encode(text).tolist()\n",
        "\n",
        "    @staticmethod\n",
        "    def distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\") -> List[float]:\n",
        "        similarities = cosine_similarity([query_embedding], embeddings)[0]\n",
        "        return [1 - sim for sim in similarities]\n",
        "\n",
        "    @staticmethod\n",
        "    def indices_of_nearest_neighbors_from_distances(distances: List[float]) -> List[int]:\n",
        "        return sorted(range(len(distances)), key=lambda i: distances[i])\n",
        "\n",
        "def create_chroma_db(documents: List[str], path: str, name: str):\n",
        "    client = chromadb.PersistentClient(path=path)\n",
        "    collection = client.get_or_create_collection(name=name)\n",
        "    utils = EmbeddingUtils()\n",
        "    embeddings = [utils.get_embedding(doc) for doc in documents]\n",
        "    ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
        "    collection.add(documents=documents, embeddings=embeddings, ids=ids)\n",
        "    return collection\n"
      ],
      "metadata": {
        "id": "CynG-tw5b3fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_passage(query: str, db, n_results: int = 3) -> List[str]:\n",
        "    utils = EmbeddingUtils()\n",
        "    query_embedding = utils.get_embedding(query)\n",
        "    result = db.query(query_embeddings=[query_embedding], n_results=n_results)\n",
        "    return result[\"documents\"][0]\n"
      ],
      "metadata": {
        "id": "YQUIuER4b48t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_rag_prompt(query: str, relevant_passage: List[str]) -> str:\n",
        "    context = \"\\n\\n\".join(relevant_passage)\n",
        "    prompt = (\n",
        "        f\"Based on the following information:\\n\\n\"\n",
        "        f\"{context}\\n\\n\"\n",
        "        f\"Please answer the following question:\\n{query}\"\n",
        "    )\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "PGusNOApb6W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ¨¡æ“¬æ–‡å­—ç‰‡æ®µ\n",
        "chunks = [\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"Machine learning is a technique in AI.\",\n",
        "    \"Neural networks mimic the human brain.\"\n",
        "]\n",
        "\n",
        "# å»ºç«‹è³‡æ–™åº«\n",
        "db_path = \"/content/chroma_db\"\n",
        "collection_name = \"pdf_chunks\"\n",
        "collection = create_chroma_db(chunks, db_path, collection_name)\n",
        "\n",
        "# å•é¡Œèˆ‡æª¢ç´¢\n",
        "query = \"What is the connection between AI and the brain?\"\n",
        "relevant = get_relevant_passage(query, collection, n_results=2)\n",
        "prompt = make_rag_prompt(query, relevant)\n",
        "\n",
        "# é¡¯ç¤ºçµæœ\n",
        "print(\"ğŸ” å•é¡Œï¼š\", query)\n",
        "print(\"\\nğŸ“š ç›¸é—œæ®µè½ï¼š\")\n",
        "for i, p in enumerate(relevant): print(f\"Chunk {i+1}: {p}\")\n",
        "print(\"\\nğŸ“ Promptï¼š\\n\", prompt)\n"
      ],
      "metadata": {
        "id": "mNNu-DbMb7f_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}